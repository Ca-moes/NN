{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model of a neuron and the learning process\n",
    "\n",
    "## 1.1 Model of a neuron\n",
    "\n",
    "Write a Python function, which calculates the output of a neuron. Assume a model of a neuron shown in Figure 1 with three inputs and a threshold. The threshold can be interpreted as an additional input with fixed input of $-1$ and weight $w_{10}$. The output of the function has to correspond to output of the neuron.\n",
    "\n",
    "![Model of a neuron](img/model.png)\n",
    "<center>Figure 1. Model of a neuron</center>\n",
    "\n",
    "Use the scalar product of input vector $[x_0 x_1 x_2 x_3]$ and weights $[x_0 x_1 x_2 x_3]$ in order to calculate the neuron activation. The function has to have an additional input, which is used to select di\u000b",
    "erent nonlinear activation functions. The function should support the following nonlinear functions:\n",
    "\n",
    "1. Step function\n",
    "2. Piecewise linear function (ramp)\n",
    "3. Sigmoid function defined as $\\phi = \\frac{1}{1+\\exp (-av)}$, with $a=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    if (x<0):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def ramp_function(x):\n",
    "    return np.max([0, x])\n",
    "\n",
    "def sigmoid_function(x, a=1):\n",
    "    return 1.0/(1+np.exp(-a*x))\n",
    "\n",
    "def neuron(x, w, activation=lambda a:a):\n",
    "    return activation(np.dot(np.concatenate([[-1], x]), w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Pick a random weight vector $\\mathbf{w}$. Write down the chosen weights and calculate the neuron response for following inputs (for each activation function):\n",
    "\n",
    "$x_1 = [0.5, 1, 0.7]^T$\n",
    "\n",
    "$x_2 = [0, 0.8, 0.2]^T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=np.random.rand(4)\n",
    "\n",
    "x1=np.array([0.5, 1, 0.7])\n",
    "x2=np.array([0, 0.8, 0.2])\n",
    "\n",
    "print(neuron(x1, w, step_function))\n",
    "print(neuron(x2, w, step_function))\n",
    "print()\n",
    "\n",
    "print(neuron(x1, w, ramp_function))\n",
    "print(neuron(x2, w, ramp_function))\n",
    "print()\n",
    "\n",
    "print(neuron(x1, w, sigmoid_function))\n",
    "print(neuron(x2, w, sigmoid_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Three neuron network\n",
    "\n",
    "Write a function for a three neuron network (Figure 2) using the function developed in section 1.1.. Assume that neurons use the sigmoid transfer function, where $a=1$ and assume the weights are given as follows:\n",
    "\n",
    "$w_1=[1, 0.5, 1, -0.4]$\n",
    "\n",
    "$w_2=[0.5, 0.6, -1.5, -0.7]$\n",
    "\n",
    "$w_3=[-0.5, -1.5, 0.6]$\n",
    "\n",
    "*Remark*: The first element of the weight vector is the threshold of a neuron and is shown as $w_{i0}$ in Figure 2.\n",
    "\n",
    "![Three neuron network](img/three.png)\n",
    "<center>Figure 2. Three neuron network</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def three_neuron_network(x):\n",
    "    w1=np.array([1, 0.5, 1, -0.4])\n",
    "    w2=np.array([0.5, 0.6, -1.5, -0.7])\n",
    "    w3=np.array([-0.5, -1.5, 0.6])\n",
    "    \n",
    "    neuron_one=neuron(x, w1, sigmoid_function)\n",
    "    neuron_two=neuron(x, w2, sigmoid_function)\n",
    "    \n",
    "    neuron_three=neuron(np.concatenate([[neuron_one], [neuron_two]]), w3, sigmoid_function)\n",
    "    \n",
    "    return neuron_three    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. What is the output of the network for input given as $x=[0.3, 0.7, 0.9]^T$?\n",
    "2. Does the output of the network depend on neuron weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.array([0.3, 0.7, 0.9])\n",
    "print(three_neuron_network(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Delta rule\n",
    "\n",
    "The goal of this experiment is to better understand the learning process. In this experiment we will implement a logical AND function using one neuron with two inputs and a threshold (see Figure 3). We will use the sigmoid nonlinear activation function with $a=1$.\n",
    "\n",
    "For the learning phase, we have to define the following input output pairs $x_i$, $y_i$ for the logical AND function: for inputs $x_1=[-1, 0, 0]^T$, $x_2=[-1, 0, 1]^T$, and $x_3=[-1, 1, 0]^T$ the output $y$ should be equal to $0$; for input vector $x_4=[-1, 1, 1]^T$ the output value $y$ should be equal to $1$. The first component of all input vectors has value $-1$ and defines the neuron threshold visible as $w_{10}$ in the Figure. At the beginning we set the neuron weights to random values. We use the delta rule in order to update the weights:\n",
    "\n",
    "$$\\Delta w_{kj}=\\eta e_{k}(n)x_{j}(n)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$e_{k}(n)=d_{k}(n)-y_{k}(n)$$\n",
    "\n",
    "where $d_{k}(n)$ is the expected neuron output and $y_{k}(n)$ is the obtained neuron output. This iterative procedure is repeated until the error is sufficiently small.\n",
    "\n",
    "![One neuron network](img/one.png)\n",
    "<center>Figure 3. One neuron network</center>\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Experiment with different starting weights and different learning rates. (In case of instabilities, perform the experiment using a small learning rate, for example $\\eta =0.05$). Show the error function ($y$-axis) and number of iterations ($x$-axis) for different learning rates.\n",
    "\n",
    "    - What is the best learning rate? How does the learning rate affect the neural network?\n",
    "    - How did you define the sufficiently small error used to terminate the algorithm?\n",
    "    - After how many iterations does the algorithm terminate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1=[-1, 0, 0]\n",
    "x2=[-1, 0, 1]\n",
    "x3=[-1, 1, 0]\n",
    "x4=[-1, 1, 1]\n",
    "\n",
    "y1=y2=y3=0\n",
    "y4=1\n",
    "\n",
    "X=np.array([x1, x2, x3, x4])\n",
    "Y=np.array([y1, y2, y3, y4])\n",
    "\n",
    "w=np.random.rand(3)\n",
    "\n",
    "eta=0.05\n",
    "error_threshold=7e-2\n",
    "errors=[];\n",
    "\n",
    "while True:\n",
    "    mse=0\n",
    "    for x, y in zip(X, Y):\n",
    "        y_predicted=np.dot(x, w)\n",
    "        e=y-y_predicted\n",
    "        mse+=e*e\n",
    "        \n",
    "        dw=eta*e*x\n",
    "        w+=dw\n",
    "    mse/=X.shape[0]\n",
    "    \n",
    "    errors.append(mse)\n",
    "    if (mse<error_threshold):\n",
    "        break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
