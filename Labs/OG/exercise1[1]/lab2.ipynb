{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Associative memory\n",
    "\n",
    "## 2.1 Forming the correlation matrix directly\n",
    "\n",
    "In this part of the exercise we will use the direct approach in forming the correlation matrix. Memory based on the correlation matrix should memorize input-output association pairs represented as vectors. For each input vector (key) the memory has to memorize the output pattern i.e. vector in an ASCII code formulation. In this example we will use 4-dimensional input and output vectors. Words (output vectos) that have to be memorized are: '*vrat*' , '*kraj*' , '*cres*' , '*otac*'. Vectors $b_i$, which represent those words should be formed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "real=lambda x: np.array([[ord(character) for character in x]]).T\n",
    "\n",
    "b1=real(\"vrat\")\n",
    "b2=real(\"kraj\")\n",
    "b3=real(\"cres\")\n",
    "b4=real(\"otac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Orthogonal input vectors\n",
    "\n",
    "This experiment demonstrates how to create an associative memory. Ortonormalized set of vectors defined as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = np.array([[1, 0, 0, 0]]).T\n",
    "a2 = np.array([[0, 1, 0, 0]]).T\n",
    "a3 = np.array([[0, 0, 1, 0]]).T\n",
    "a4 = np.array([[0, 0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is used as input vector set (set of keys). We form the memory correlation matrix $\\mathbf{M}$ using input output pairs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify whether the memory is functioning properly, we have to calculate outputs for each input vector. For example, the output for the key $a_1$ can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char=lambda x:\"\".join(map(chr, map(int, list(x))))\n",
    "\n",
    "word=char(M@a1)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. What is the response for each key? Were all input-output pairs memorized correctly?\n",
    "2. How many input-output pairs would be memorized if vectors $a_i$ were not normalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [a1, a2, a3, a4]:\n",
    "    print(k.T, char(M@k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Correlation matrix properties\n",
    "\n",
    "The goal of this experiment is to demonstrate the capacity of obtained memory. In this part of the exercise we will try to memorize one more (fifth) word ('*mrak*'). In 4-dimensional vector space the maximum number of linearly independent vectors is four. Because of this fact, we pick an arbitrary unit vector as the fifth key, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a5 = (a1 + a3) / np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form vectors $b_5$ ('*mrak*') and $a_5$ as explained and add them into the memory using the following expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b5 = real(\"mrak\")\n",
    "M_five = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T + b5 * a5.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Was the new association properly memorized?\n",
    "2. Did other associations stay correctly memorized?\n",
    "    - If not - which were not memorized correctly and why?\n",
    "    - If yes - which were memorized correctly and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [a1, a2, a3, a4]:\n",
    "    before=char(M@k)\n",
    "    after=char(M_five@k)\n",
    "    print(k.T, before, after)\n",
    "\n",
    "print(a5.T, char(M_five@a5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Word pairs as associations\n",
    "\n",
    "In this experiment we will form the associative memory, which memorizes word pairs. The associations, which have to be memorized are: *ruka*-*vrat*, *kset*-*kraj*, *more*-*cres*, *mama*-*otac*. Generate input vectors (keys) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = real(\"ruka\")\n",
    "a2 = real(\"kset\")\n",
    "a3 = real(\"more\")\n",
    "a4 = real(\"mama\")\n",
    "M = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors bi don't have to be created again because they are the ones used in the first part of the exercise. Form the matrix M using the same procedure as in the first part of the exercise.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. What is the response for each input key?\n",
    "2. Which associations were memorized correctly?\n",
    "3. Which associations were not memorized correctly and why?\n",
    "4. How can we fix this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)\n",
    "for k in [a1, a2, a3, a4]:\n",
    "    print(k, char(M@k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Input vector orthogonalization\n",
    "\n",
    "In this experiment we show an associative memory, which uses keys that are orthonormalized. We use the Gram-Schmidt orthogonalization method as follows. We first form the matrix $\\mathbf{A}$ using vectors $a_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=np.hstack([a1, a2, a3, a4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we perform the orthonormalization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import orth\n",
    "C=orth(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract individual orthonormal vectors $c_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1=np.array([C[0]]).T\n",
    "c2=np.array([C[1]]).T\n",
    "c3=np.array([C[2]]).T\n",
    "c4=np.array([C[3]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we form a new matrix $\\mathbf{M}$ using vectors $c_i$ instead of vectors $a_i$ when creating the matrix $\\mathbf{M}$. Verify the responses of matrix $\\mathbf{M}$ with vectors $c_i$ as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = b1 * c1.T + b2 * c2.T + b3 * c3.T + b4 * c4.T\n",
    "for c in [c1, c2, c3, c4]:\n",
    "    print(c.T, char(M@c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. What is the effect of vector orthonormalization?\n",
    "2. How many pairs were correctly memorized?\n",
    "3. What can we expect when normalizing the vectors?\n",
    "4. What can we expect when only orthogonalizing the vectors?\n",
    "5. What can we expect if vectors $c_i$ are linearly independent but not orthogonal?\n",
    "\n",
    "### 2.1.5 Finding the correlation matrix using matrix inversion\n",
    "\n",
    "For previously used word pairs (*ruka*-*vrat*, *kset*-*kraj*, *more*-*cres*, *mama*-*otac*) find a $4\\times 4$ correlation matrix $\\mathbf{M}$ as $\\mathbf{M} = \\mathbf{B}\\mathbf{A}^{-1}$, where matrix $\\mathbf{B}$ is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B=np.hstack([b1, b2, b3, b4])\n",
    "M=B@np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Were all associations properly memorized? Remark: The result should be rounded to the nearest number before comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [a1, a2, a3, a4]:\n",
    "    print(a.T, char(np.round(M@a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Finding the correlation matrix using pseudo-inversion\n",
    "\n",
    "A pseudo-inverse matrix can be used in order to find the correlation matrix when number of associations is larger than dimensionality of vectors representing the associations. In this case, the correlation matrix can be found as $\\mathbf{M} = \\mathbf{B}\\mathbf{A}^{+}$, where $\\mathbf{A}^{+}$ is a pseudo-inverse matrix defined as $\\mathbf{A}^{+} = \\mathbf{A}^{T}(\\mathbf{A}\\mathbf{A}^{T})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the vectors $a_i$ and $b_i$ are defined previously (five associations in total). Find the pseudo-inverse matrix for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = np.array([[1, 0, 0, 0]]).T\n",
    "a2 = np.array([[0, 1, 0, 0]]).T\n",
    "a3 = np.array([[0, 0, 1, 0]]).T\n",
    "a4 = np.array([[0, 0, 0, 1]]).T\n",
    "\n",
    "a5 = (a1 + a3) / np.sqrt(2)\n",
    "\n",
    "A=np.hstack([a1, a2, a3, a4, a5])\n",
    "B=np.hstack([b1, b2, b3, b4, b5])\n",
    "A_pseudo=A.T@np.linalg.inv(A@A.T)\n",
    "M=B@A_pseudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Were all pairs memorized correctly?\n",
    "2. If not, what is the error between expected and obtained values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [a1, a2, a3, a4, a5]:\n",
    "    print(a.T, char(np.round(M@a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Finding the correlation matrix using supervised learning\n",
    "\n",
    "This experiment shows us how to form the matrix $\\mathbf{M}$ using supervised learning. In two following experiments we will use learning with error correction.\n",
    "\n",
    "### 2.2.1 Learning with error correction\n",
    "\n",
    "Form matrices $\\mathbf{A}$ and $\\mathbf{B}$ where each contains 4 vectors stacked in columns as explained in previous experiments. Check the contents of obtained matrices with following operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1=real(\"ruka\")\n",
    "a2=real(\"kset\")\n",
    "a3=real(\"more\")\n",
    "a4=real(\"mama\")\n",
    "\n",
    "b1=real(\"vrat\")\n",
    "b2=real(\"kraj\")\n",
    "b3=real(\"cres\")\n",
    "b4=real(\"otac\")\n",
    "\n",
    "A=np.hstack([a1, a2, a3, a4])\n",
    "B=np.hstack([b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start the learning procedure we have to initialize the matrix $\\mathbf{M}$ (For example, random values uniformly generated in $[-0.5, 0.5]$ interval):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M=np.random.rand(4, 4)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the learning part use the function *trainlms*, which is the implementation of the Widrow-Hoff LMS learning algorithm. The function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainlms(A, B, M, ni, max_num_iter, min_err=0.02):\n",
    "    d=B\n",
    "    x=A\n",
    "    w=M\n",
    "    \n",
    "    n=0\n",
    "    err=[]\n",
    "    while (n<max_num_iter):\n",
    "        n+=1\n",
    "        e=d-w@x\n",
    "        w+=ni*np.dot(e, x.T)\n",
    "        err.append(np.sum(np.sum(np.multiply(e, e))))\n",
    "        if (err[-1]<min_err):\n",
    "            break\n",
    "    return w, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where *max_num_iter* is the number of iterations and *ni* is the learning rate. Find the *max_num_iter* variable experimentally. For *ni* you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ni=0.9999/np.linalg.eig(A @ A.T)[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function trainlms performs the learning until SSE drops below $0.02$ or maximum number of iterations is performed. After the learning phase, look at the responses of the correlation matrix $\\mathbf{M}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M, e=trainlms(A, B, M, ni, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(M@A)==B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will see, which characters were properly reconstructed: the positions with correct reconstructions will have value *True* and  other positions will have value *False*. By calling the *trainlms* multiple times we can extend the learning process and maybe increase the number of memorized characters but the proper way to extend the learning process is to increase the *max_num_iter* variable. We can draw a graph, which plots the error with number of iterations (in logaritmic scale) using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(e)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. Plot a graph showing number of memorized characters tied to number of used iterations. (Caution: When building the graph, start the simulation with the same starting matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_letters=[]\n",
    "M_initial=np.random.rand(4, 4)-0.5\n",
    "for i in range(1, 40000, 25):\n",
    "    M, e=trainlms(A, B, M_initial, ni, i)\n",
    "    num_letters.append(np.sum(np.round(np.dot(M, A))==B))\n",
    "\n",
    "plt.plot(list(range(1, 40000, 25)), num_letters)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Number of correct letters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Effect of larger number of associations\n",
    "\n",
    "This experiment demonstrates the capacity of the associative memory. What is the capacity of a $4\\times 4$ correlation matrix based associative memory?\n",
    "\n",
    "For additional pair '*auto*'-'*mrak*' create vectors $a_5$ and $b_5$ as explained in the previous part of the exercise. Create new matrices A and B with dimensions $4$ (rows) $\\times$ $5$ (columns) in the same way as previously explained. Initialize the matrix $\\mathbf{M}$ with random starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a5=real(\"auto\")\n",
    "b5=real(\"mrak\")\n",
    "\n",
    "A=np.hstack([a1, a2, a3, a4, a5])\n",
    "B=np.hstack([b1, b2, b3, b4, b5])\n",
    "\n",
    "M=np.random.rand(4, 4)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *trainlms* function in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni=0.9999/max(np.linalg.eig(np.dot(A, A.T))[0])\n",
    "M, e=trainlms(A, B, M, ni, 100000)\n",
    "print(np.sum(np.round(np.dot(M, A))==B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. How many iterations did you use?\n",
    "2. How many characters were memorized correctly?\n",
    "3. What is the SSE error?\n",
    "4. What happens if we call the function from the beginning?\n",
    "5. How many characters are correctly memorized now and how large is the mistake? Is there any difference and why?\n",
    "6. Is it possible to train this network in order to memorize all five associations?\n",
    "7. Why? (Explain the previous answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
