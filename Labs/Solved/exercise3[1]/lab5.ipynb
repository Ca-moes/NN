{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"h8RG3O_v_s_N"},"source":["# Exercise 5: Multilayer perceptron\n","\n","The focus of this exercise will be on multilayer perceptron and to do that in a simpler way, an introduction to TensorFlow is given first.\n","\n","## 5.1 Introduction to TensorFlow\n","\n","TensorFlow is an open-source symbolic math software library used for machine learning applications such as neural networks. The following command is used to import TensorFlow in the Python code:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5416,"status":"ok","timestamp":1670843702189,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"7jRbVXYp_s_b","outputId":"5b5bf23a-50bc-4e41-8f03-e7c7b501f411"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["# https://stackoverflow.com/questions/55142951/tensorflow-2-0-attributeerror-module-tensorflow-has-no-attribute-session\n","import tensorflow as tf2 # imports v2\n","\n","## import v1\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"]},{"cell_type":"markdown","metadata":{"id":"YycVsLdY_s_e"},"source":["### 5.1.1 Constants, sessions, and operations\n","TensorFlow is based around tensors - *n*-dimensional arrays of a given type. Three main tensor types in TensorFlow are constant, variable, and placeholder. To create a constant, the [tf.constant()](https://www.tensorflow.org/api_docs/python/tf/constant) method is used:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99,"status":"ok","timestamp":1670843702191,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"MF6lLr_9_s_g","outputId":"83659265-2637-4085-ca20-0912a26fce3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor(\"Const:0\", shape=(), dtype=int32)\n"]}],"source":["c=tf.constant(2)\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"ZyKpsNhZ_s_i"},"source":["This constant tensor like other tensors has a value, a shape, a data type, and a name. These can be directly specified:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1670843702192,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"gauVoaRV_s_m","outputId":"8060547d-edf8-4c95-c82a-7bdb8748201a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor(\"our_constant:0\", shape=(2, 5), dtype=float32)\n"]}],"source":["c=tf.constant(3, shape=(2, 5), dtype=tf.float32, name=\"our_constant\")\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"vazrCVDh_s_n"},"source":["To evaluate a tensor, a [Session](https://www.tensorflow.org/api_docs/python/tf/Session) instance is required. Sessions are environments where tensors and operations are executed. A session can be created and then used for evaluation as follows:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84,"status":"ok","timestamp":1670843702193,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"U1XdU_YZ_s_o","outputId":"729c4bf2-fe6c-40d3-84cd-92c4ce2eb7c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[3. 3. 3. 3. 3.]\n"," [3. 3. 3. 3. 3.]]\n"]}],"source":["session=tf.compat.v1.Session()\n","print(session.run(c))"]},{"cell_type":"markdown","metadata":{"id":"Lrsvno-h_s_q"},"source":["Other useful ways of creating constant tensors include the methods [tf.zeros()](https://www.tensorflow.org/api_docs/python/tf/zeros) and [tf.ones()](https://www.tensorflow.org/api_docs/python/tf/ones):"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1670843702194,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"o9ha4p53_s_r","outputId":"5b0f8a66-bd04-4373-ac9d-25cef0e86dfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0.]\n"," [0. 0. 0.]]\n","[[1.]\n"," [1.]\n"," [1.]]\n"]}],"source":["z=tf.zeros((2, 3))\n","print(session.run(z))\n","\n","o=tf.ones((3, 1))\n","print(session.run(o))"]},{"cell_type":"markdown","metadata":{"id":"w1jar7Mw_s_s"},"source":["The most common methods to create tensors with random values are [tf.random_uniform](https://www.tensorflow.org/api_docs/python/tf/random_uniform) and [tf.random_normal](https://www.tensorflow.org/api_docs/python/tf/random_normal):"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1670843702195,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"0OhMI43M_s_t","outputId":"6f667127-bdce-4a4c-98bb-867863f30275"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[2.52144   3.909486  6.001857  4.087057 ]\n"," [5.3629565 5.9860244 5.44572   4.0153193]]\n","[[-0.8800217  -1.0065781   0.49401343  0.10883627]\n"," [-0.25338662 -0.17100023  1.9540884  -0.36700243]]\n"]}],"source":["u=tf.random_uniform(shape=(2, 4), minval=2, maxval=7)\n","print(session.run(u))\n","\n","n=tf.random_normal(shape=(2, 4), mean=0, stddev=1)\n","print(session.run(n))"]},{"cell_type":"markdown","metadata":{"id":"B4LbJO_Z_s_u"},"source":["Applying addition, subtraction, multiplication, and division to tensors can be achieved by using operators +, -, *, and / or by calling  [tf.add()](https://www.tensorflow.org/api_docs/python/tf/add), [tf.subtract()](https://www.tensorflow.org/api_docs/python/tf/subtract), [tf.multiply()](https://www.tensorflow.org/api_docs/python/tf/multiply), [tf.divide()](https://www.tensorflow.org/api_docs/python/tf/divide). Each of these operations is performed element-wise, e.g. when applied to matrices, [tf.multiply()](https://www.tensorflow.org/api_docs/python/tf/multiply) is not matrix multiplication like [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/matmul), but element-wise multiplication."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1670843702196,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"_zNaOmmK_s_v","outputId":"7771733e-8401-4e39-c089-68757a7f2129"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 1.]]\n","[[2. 2.]]\n","[[8. 8.]]\n","[[-1. -1.]]\n","[[2. 2.]]\n","[[0.5 0.5]]\n","[[2. 2.]\n"," [2. 2.]]\n"]}],"source":["a=tf.ones((1, 2))\n","b=2*tf.ones((1, 2))\n","\n","print(session.run(a))\n","print(session.run(b))\n","print(session.run(a+b+5))\n","print(session.run(tf.subtract(a, b)))\n","print(session.run(a*b))\n","print(session.run(tf.divide(a, b)))\n","\n","print(session.run(tf.matmul(tf.transpose(a), b)))"]},{"cell_type":"markdown","metadata":{"id":"ZLFWYaVF_s_w"},"source":["Some other operations include [tf.abs()](https://www.tensorflow.org/api_docs/python/tf/abs), [tf.exp()](https://www.tensorflow.org/api_docs/python/tf/exp), [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/matmul), [tf.pow()](https://www.tensorflow.org/api_docs/python/tf/pow), [tf.square()](https://www.tensorflow.org/api_docs/python/tf/square), [tf.transpose()](https://www.tensorflow.org/api_docs/python/tf/transpose)."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1670843702196,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"dd_gV4PE_s_w","outputId":"a3dc3ed1-c3b2-43eb-ddf6-aa721f457f38"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.21065234]\n"," [0.09052168]\n"," [0.7195048 ]\n"," [1.2277384 ]\n"," [1.7690411 ]]\n"]}],"source":["print(session.run(tf.transpose(tf.abs(tf.random_normal((1, 5), mean=0, stddev=3)))))"]},{"cell_type":"markdown","metadata":{"id":"3BgheG4E_s_x"},"source":["### 5.1.2 Placeholders and variables\n","Tensors used for more complex data and computation are [placeholders](https://www.tensorflow.org/api_docs/python/tf/placeholder) and [variables](https://www.tensorflow.org/api_docs/python/tf/Variable). Placeholders are providers of future values and mostly serve to take the input to the network. For this reasong thay cannot be directly evaluated unless its value is \"fed\" i.e. given by means of dictionary."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1670843702198,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"tEVcQMxs_s_2","outputId":"5db612bb-2993-4315-e485-30809bfd59f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor(\"Placeholder:0\", dtype=float32)\n","session.run a, a=-5\t\t -5.0\n","Tensor(\"Placeholder:0\", dtype=float32)\n","Tensor(\"Abs_1:0\", dtype=float32)\n","session.run b, a=-5\t\t 5.0\n","session.run b, a=-17,1,-2\t [17.  1.  2.]\n","\n","\n","[[1. 2. 3.]\n"," [2. 4. 6.]\n"," [3. 6. 9.]]\n","[[14.]]\n"]}],"source":["#we create a placeholder\n","a=tf.placeholder(dtype=tf.float32)\n","print(a)\n","#this would produce an error\n","# print(session.run(a))\n","\n","#but not this\n","print('session.run a, a=-5\\t\\t', session.run(a, feed_dict={a:-5}))\n","print(a)\n","#we use its value later\n","b=tf.abs(a)\n","print(b)\n","\n","#to provide a value to the placeholder, feed_dict is used\n","print('session.run b, a=-5\\t\\t', session.run(b, feed_dict={a:-5}))\n","#we can also use different input size\n","print('session.run b, a=-17,1,-2\\t', session.run(b, feed_dict={a:[-17, 1, -2]}))\n","\n","print(\"\\n\")\n","\n","m1=tf.placeholder(dtype=tf.float32)\n","m2=tf.placeholder(dtype=tf.float32)\n","\n","p=tf.matmul(m1, m2)\n","print(session.run(p, feed_dict={m1:[[1], [2], [3]], m2:[[1, 2, 3]]}))\n","print(session.run(p, feed_dict={m1:[[1, 2, 3]], m2:[[1], [2], [3]]}))\n"]},{"cell_type":"markdown","metadata":{"id":"xXW0TGQY_s_4"},"source":["Variable are mostly used for trainable parameters. While constants are intialized when created,variables are initialized within the session by a procedure that must be defined. Manual assignment of new values tan be is possible using [tf.assign()](https://www.tensorflow.org/api_docs/python/tf/assign). Variables are mostly changed during the optimization process.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1670843702198,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"Io1eRpP8_s_4","outputId":"32ded44e-9374-415d-ce9b-bf55fc247f32"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n","[[0.40200806 0.4202485  0.43005693]\n"," [0.75940263 0.8236791  0.91975236]]\n"]}],"source":["session=tf.Session()\n","#a constant is used for the initialization procedure\n","a=tf.Variable(3)\n","#random values will be used for initialization\n","b=tf.Variable(tf.random_uniform(shape=(2, 3)))\n","\n","#this would produce an error since the variable has not been initialized - only the initialization procedure has been defined\n","#print(session.run(a))\n","\n","#initialize all variables\n","session.run(tf.global_variables_initializer())\n","#now evaluate the variable\n","print(session.run(a))\n","print(session.run(b))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"mlQrBOtU_s_5"},"source":["### 5.1.3 Linear regression\n","To have an example of a fully written network, let's now implement simple multivariate linear regression using TensorFlow. The used model will be $y=\\mathbf{w}^{T}\\mathbf{x}+\\mathbf{b}$."]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"executionInfo":{"elapsed":28,"status":"ok","timestamp":1670843702199,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"-zWE9G-X_s_6"},"outputs":[],"source":["#data placeholders - this will be used for the given features and for the ground-truth value of y\n","x=tf.placeholder(dtype=tf.float32, shape=[None, 3])\n","y=tf.placeholder(dtype=tf.float32, shape=[None, 1])\n","\n","#parameter variables\n","w=tf.Variable(tf.random_normal(shape=(3, 1)))\n","b=tf.Variable(tf.random_normal([1, 1]))\n","\n","#the model for y - this will be used for the predicted value of y\n","y_predicted=tf.matmul(x, w)+b"]},{"cell_type":"markdown","metadata":{"id":"J8P4bdND_s_6"},"source":["TensorFlow trains a model i.e. learns its parameter values by minimizing a loss function that needs to be defined. The minimization is carried out by a defined optimizer object by calling its [minimize()](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize) method called. The learning rate chosen when defining the optimizer objects and the number of training epochs will have a significant impact on the model training process by influencing how fast the learning process will converge."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1670843702885,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"ZBeape1c_s_7","outputId":"c6c8e043-e98e-4b91-cab8-a928a6c35332"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch #10: 0.16051327\n","Epoch #20: 0.0032078137\n","Epoch #30: 7.950544e-05\n","Epoch #40: 2.0719253e-06\n","Epoch #50: 5.46065e-08\n","Epoch #60: 1.4392111e-09\n","Epoch #70: 3.8453695e-11\n","Epoch #80: 1.3559262e-12\n","Epoch #90: 1.9294521e-13\n","Epoch #100: 1.8590809e-13\n","[[ 0.9999999]\n"," [ 2.9999998]\n"," [-1.9999998]]\n"]}],"source":["#the loss function will be mean square\n","loss=tf.reduce_mean(tf.square(y_predicted-y))\n","\n","#gradient descent optimizer with learning rate 0.1\n","optimizer=tf.train.GradientDescentOptimizer(0.1)\n","\n","#train operation\n","train=optimizer.minimize(loss)\n","\n","#generation data for regression\n","import numpy as np\n","w_real=np.array([[1], [3], [-2]])\n","x_train=np.random.normal(size=(100, 3))  # 100 input examples of size 3  \n","y_train=(w_real.T@x_train.T).T;  # 100 expect output\n","\n","session.run(tf.global_variables_initializer())\n","for epoch in range(100):\n","    session.run(train, feed_dict={x:x_train, y:y_train})\n","    if ((epoch+1)%10==0):\n","        print(\"Epoch #\"+str(epoch+1)+\": \"+str(session.run(loss, feed_dict={x:x_train, y:y_train})))\n","\n","#print the trained weights\n","print(session.run(w))"]},{"cell_type":"markdown","metadata":{"id":"Xu5AvW8E_s_8"},"source":["## 5.2 The XOR problem\n","XOR samples are not linearly separable. However, they can be separated by introducing non-linearities. In TensorFlow some of them include [tf.sigmoid()](https://www.tensorflow.org/api_docs/python/tf/sigmoid), [tf.tanh()](https://www.tensorflow.org/api_docs/python/tf/tanh), [tf.nn.relu()](https://www.tensorflow.org/api_docs/python/tf/nn/relu), etc. Besides the common [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer), there are other optimizers as well, e.g. [tf.train.AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer).\n","\n","**Task**\n","\n","Below is the code for solving the XOR problem in TensorFlow. Determine how many epochs are required for the training process to converge for each combination of the chosen activation function, optimizer, and various learning rates. Which combination turned out to be the best?\n","\n","**Results**\n","\n","| Activation \t| Optimizer \t| Learning_rate \t| Epochs \t| Error    \t|\n","|:----------:\t|:---------:\t|:-------------:\t|:------:\t|----------\t|\n","| Sigmoid    \t| Gradient  \t| 0.1           \t| 2763   \t| 9.94e-05 \t|\n","| Tanh       \t| Gradient  \t| 0.1           \t| 898    \t| 9.72e-05 \t|\n","| ReLu       \t| Gradient  \t| x             \t| 10000  \t| 0.25     \t|\n","| Sigmoid    \t| Adam      \t| 0.1           \t| 10000  \t| 0.16     \t|\n","| Tanh       \t| Adam      \t| 0.1           \t| 126    \t| 7.54e-05 \t|\n","| ReLu       \t| Adam      \t| x             \t| 10000  \t| 0.25     \t|\n","|            \t|           \t|               \t|        \t|          \t|\n","| Tanh       \t| Adam      \t| 0.01          \t| 621    \t| 9.90e-5  \t|\n","| Sigmoid    \t| Adam      \t| 0.01          \t| 457    \t| 9.8e-05  \t|\n","| Sigmoid    \t| Adam      \t| 0.01          \t| 10000  \t| 0.25     \t|\n","| Sigmoid    \t| Gradient  \t| 0.01          \t| 10000  \t| 0.25     \t|\n","| Sigmoid    \t| Gradient  \t| 0.001         \t| 10000  \t| 0.25     \t|\n","\n","\n","Best combination was Tanh - Adam - 0.1\n","\n","ReLu kept giving 10000 epochs and a stable error"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17420,"status":"ok","timestamp":1670843720283,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"a4OYCh_I_s_9","outputId":"1dc38c95-d213-4511-d0b3-f48bd6012624"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch #100: 0.24712977\n","Epoch #200: 0.24692926\n","Epoch #300: 0.2468443\n","Epoch #400: 0.24675892\n","Epoch #500: 0.24667299\n","Epoch #600: 0.24658644\n","Epoch #700: 0.24649931\n","Epoch #800: 0.24641153\n","Epoch #900: 0.24632293\n","Epoch #1000: 0.24623364\n","Epoch #1100: 0.24614346\n","Epoch #1200: 0.2460525\n","Epoch #1300: 0.24596055\n","Epoch #1400: 0.2458677\n","Epoch #1500: 0.24577382\n","Epoch #1600: 0.24567887\n","Epoch #1700: 0.24558282\n","Epoch #1800: 0.24548565\n","Epoch #1900: 0.24538732\n","Epoch #2000: 0.24528773\n","Epoch #2100: 0.24518684\n","Epoch #2200: 0.24508463\n","Epoch #2300: 0.24498102\n","Epoch #2400: 0.24487603\n","Epoch #2500: 0.24476951\n","Epoch #2600: 0.24466152\n","Epoch #2700: 0.2445519\n","Epoch #2800: 0.24444067\n","Epoch #2900: 0.24432781\n","Epoch #3000: 0.2442132\n","Epoch #3100: 0.24409682\n","Epoch #3200: 0.24397859\n","Epoch #3300: 0.24385847\n","Epoch #3400: 0.24373648\n","Epoch #3500: 0.24361245\n","Epoch #3600: 0.24348639\n","Epoch #3700: 0.24335825\n","Epoch #3800: 0.24322793\n","Epoch #3900: 0.2430954\n","Epoch #4000: 0.2429606\n","Epoch #4100: 0.2428235\n","Epoch #4200: 0.24268398\n","Epoch #4300: 0.24254203\n","Epoch #4400: 0.24239762\n","Epoch #4500: 0.24225059\n","Epoch #4600: 0.24210092\n","Epoch #4700: 0.24194857\n","Epoch #4800: 0.24179348\n","Epoch #4900: 0.24163556\n","Epoch #5000: 0.24147475\n","Epoch #5100: 0.24131097\n","Epoch #5200: 0.24114415\n","Epoch #5300: 0.24097428\n","Epoch #5400: 0.2408012\n","Epoch #5500: 0.24062489\n","Epoch #5600: 0.24044532\n","Epoch #5700: 0.24026233\n","Epoch #5800: 0.24007592\n","Epoch #5900: 0.23988599\n","Epoch #6000: 0.23969243\n","Epoch #6100: 0.23949523\n","Epoch #6200: 0.23929426\n","Epoch #6300: 0.23908947\n","Epoch #6400: 0.23888078\n","Epoch #6500: 0.23866808\n","Epoch #6600: 0.23845135\n","Epoch #6700: 0.23823047\n","Epoch #6800: 0.23800537\n","Epoch #6900: 0.23777598\n","Epoch #7000: 0.23754212\n","Epoch #7100: 0.2373039\n","Epoch #7200: 0.23706108\n","Epoch #7300: 0.23681363\n","Epoch #7400: 0.23656149\n","Epoch #7500: 0.23630452\n","Epoch #7600: 0.23604265\n","Epoch #7700: 0.23577586\n","Epoch #7800: 0.23550397\n","Epoch #7900: 0.23522694\n","Epoch #8000: 0.2349447\n","Epoch #8100: 0.23465714\n","Epoch #8200: 0.23436418\n","Epoch #8300: 0.23406574\n","Epoch #8400: 0.23376176\n","Epoch #8500: 0.23345217\n","Epoch #8600: 0.23313674\n","Epoch #8700: 0.2328156\n","Epoch #8800: 0.23248851\n","Epoch #8900: 0.23215541\n","Epoch #9000: 0.23181629\n","Epoch #9100: 0.23147102\n","Epoch #9200: 0.23111956\n","Epoch #9300: 0.23076177\n","Epoch #9400: 0.23039758\n","Epoch #9500: 0.23002699\n","Epoch #9600: 0.22964986\n","Epoch #9700: 0.2292661\n","Epoch #9800: 0.2288757\n","Epoch #9900: 0.22847855\n","Epoch #10000: 0.22807464\n"]}],"source":["activation_type=tf.nn.sigmoid;\n","#activation_type=tf.nn.tanh;\n","#activation_type=tf.nn.relu;\n","\n","optimizer_type=tf.train.GradientDescentOptimizer\n","#optimizer_type=tf.train.AdamOptimizer\n","\n","learning_rate=0.01;\n","\n","\n","\n","threshold=1e-4\n","\n","session=tf.Session()\n","\n","#training data\n","x_train=np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y_train=np.array([[0], [1], [1], [0]])\n","\n","x=tf.placeholder(tf.float32, [None, 2])\n","y=tf.placeholder(tf.float32, [None, 1])\n","\n","w1=tf.Variable(tf.random_uniform((2, 2)))\n","b1=tf.Variable(tf.random_uniform([2]))\n","\n","w2=tf.Variable(tf.random_uniform((2, 1)))\n","b2=tf.Variable(tf.random_uniform([1]))\n","\n","\n","f1=tf.matmul(x, w1)+b1\n","f2=activation_type(f1)\n","y_predicted=tf.matmul(f2, w2)+b2\n","\n","loss=tf.reduce_mean(tf.square(y_predicted-y))\n","\n","optimizer=optimizer_type(learning_rate)\n","train=optimizer.minimize(loss)\n","\n","session.run(tf.global_variables_initializer())\n","\n","for epoch in range(10000):\n","    session.run(train, feed_dict={x:x_train, y:y_train})\n","    error=session.run(loss,{x:x_train, y:y_train})\n","    if ((epoch+1)%100==0):\n","        print(\"Epoch #\"+str(epoch+1)+\": \"+str(error))\n","    if (error<threshold):\n","        print(\"Threshold passed at epoch #\"+str(epoch+1)+\" with error\"+str(error));\n","        break;\n","\n","session.close();"]},{"cell_type":"markdown","metadata":{"id":"4nf06RMI_s_-"},"source":["## 5.3 The MNIST dataset\n","[The MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains 60,000 training and 10,000 test images of handwritten digits. It is used to test the ability of a method to recognize which digit is on a given image. Although spatial distribution of individual image pixels matters, in this example we are going to disregard it and simply use individual pixel values as features. There are $28\\cdot 28=784$ pixels i.e. features per image. The basic code is given below.\n","\n","**Task**\n","\n","Experiment with different activation functions, learning rates, batch sizes, optimizers, and architectures. What is the best combination of them? Which of them has the highest impact on the accuracy and rate of convergence? How about the size of hidden layers? Make the comparisons and draw the appropriate plots."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":330,"status":"ok","timestamp":1670856333044,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"AHGzKXp3Dr60"},"outputs":[],"source":["import numpy\n","\n","def next_batch(X, y, batch_count, batch_size):\n","    start = batch_count * batch_size\n","    end = (batch_count + 1) * batch_size\n","    if end > len(X):\n","        end = len(X)\n","    return (X[start:end,:,:], y[start:end,:])\n","\n","\n","def combine_dims(a, start=0, count=2):\n","    \"\"\" Reshapes numpy array a by combining count dimensions, \n","        starting at dimension index start \"\"\"\n","    s = a.shape\n","    return numpy.reshape(a, s[:start] + (-1,) + s[start+count:])"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79925,"status":"ok","timestamp":1670844584029,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"g3WWwZ8X_s_-","outputId":"4c478603-6db9-4845-8237-cb91baf28da7"},"outputs":[{"name":"stdout","output_type":"stream","text":["batches_count:  600\n","[0.9273, 0.9352, 0.9448, 0.9482, 0.9463, 0.9547, 0.953, 0.959, 0.9595, 0.9479, 0.9574, 0.9544, 0.9621, 0.9617, 0.9615, 0.9624, 0.9601, 0.9592, 0.9539, 0.9592]\n"]}],"source":["## SINGLE TEST\n","import matplotlib.pyplot as plt\n","\n","#use MNIST data\n","#from tensorflow.examples.tutorials.mnist import input_data\n","#mnist=input_data.read_data_sets(\"mnist/\", one_hot=True)\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","test_images = combine_dims(test_images, start=1, count=2)\n","\n","train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=None, dtype='float32')\n","test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=None, dtype='float32')\n","\n","#settings\n","learning_rate=0.01\n","training_epochs_count=20\n","batch_size=100\n","batches_count=int(len(train_images)/batch_size)\n","\n","activation_function=tf.nn.relu\n","optimizer_type=tf.train.AdamOptimizer\n","\n","display_step=1\n","\n","#architecture\n","hidden_layer_size_1=256\n","hidden_layer_size_2=256\n","input_size=784\n","n_classes=10\n","\n","#data input\n","x=tf.placeholder(tf.float32, [None, input_size])\n","y=tf.placeholder(tf.float32, [None, n_classes])\n","\n","#weights\n","w1=tf.Variable(tf.random_normal([input_size, hidden_layer_size_1]))\n","w2=tf.Variable(tf.random_normal([hidden_layer_size_1, hidden_layer_size_2]))\n","w3=tf.Variable(tf.random_normal([hidden_layer_size_2, n_classes]))\n","\n","#biases\n","b1=tf.Variable(tf.random_normal([hidden_layer_size_1]))\n","b2=tf.Variable(tf.random_normal([hidden_layer_size_2]))\n","b3=tf.Variable(tf.random_normal([n_classes]))\n","\n","#layers\n","layer_1=activation_function(tf.add(tf.matmul(x, w1), b1))\n","layer_2=activation_function(tf.add(tf.matmul(layer_1, w2), b2))\n","y_predicted=tf.matmul(layer_2, w3)+b3\n","\n","cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predicted, labels=y))\n","optimizer=optimizer_type(learning_rate=learning_rate).minimize(cost)\n","\n","session=tf.Session();\n","session.run(tf.global_variables_initializer())\n","\n","correct_y_predictediction=tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n","accuracy=tf.reduce_mean(tf.cast(correct_y_predictediction, tf.float32))\n","\n","accuracies = []\n","\n","for epoch in range(training_epochs_count):\n","\tfor i in range(batches_count):\n","\t\tbatch_x, batch_y = next_batch(train_images, train_labels, i, batch_size)\n","\t\tbatch_x = combine_dims(batch_x, start=1, count=2)\n","\t\tsession.run(optimizer, feed_dict={x:batch_x, y:batch_y})\n","\tif ((epoch+1)%display_step==0):\n","\t\tacc = session.run(accuracy, feed_dict={x: test_images, y: test_labels})\n","\t\taccuracies.append(acc)\n","\t\t\t\n","session.close()\n","\n","print(accuracies)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":851,"status":"ok","timestamp":1670856249794,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"E7NSpJiwGoq1","outputId":"6849163c-9c60-4b76-aac1-f70202f6ceda"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9624\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","y_values = [0.9273, 0.9352, 0.9448, 0.9482, 0.9463, 0.9547, 0.953, 0.959, 0.9595, 0.9479, 0.9574, 0.9544, 0.9621, 0.9617, 0.9615, 0.9624, 0.9601, 0.9592, 0.9539, 0.9592]\n","\n","print(max(y_values))\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.plot(y_values)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4925718,"status":"ok","timestamp":1670861266985,"user":{"displayName":"André Gomes","userId":"13212468293918673327"},"user_tz":-60},"id":"jzMTRlnose7X","outputId":"c6dc3d1d-c674-4537-f0d1-a217bef55547"},"outputs":[],"source":["## MULTIPLE TEST\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import itertools\n","import matplotlib.pyplot as plt\n","\n","# Use MNIST data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","test_images = combine_dims(test_images, start=1, count=2)\n","\n","train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=None, dtype='float32')\n","test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=None, dtype='float32')\n","\n","# Settings\n","learning_rates = [0.1, 0.01, 0.001]\n","training_epochs_count=20\n","batch_sizes=[50, 100, 500]\n","\n","activation_functions=[tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\n","optimizer_types = [tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdagradOptimizer]\n","\n","display_step=1\n","\n","#architecture\n","hidden_layer_size_1=256\n","hidden_layer_size_2=256\n","input_size=784\n","n_classes=10\n","\n","options = itertools.product(*[learning_rates, batch_sizes, activation_functions, optimizer_types])\n","\n","for learning_rate, batch_size, activation_function, optimizer_type in options:\n","  print(learning_rate, batch_size, activation_function, optimizer_type)\n","\n","  #data input\n","  x=tf.placeholder(tf.float32, [None, input_size])\n","  y=tf.placeholder(tf.float32, [None, n_classes])\n","  #weights\n","  w1=tf.Variable(tf.random_normal([input_size, hidden_layer_size_1]))\n","  w2=tf.Variable(tf.random_normal([hidden_layer_size_1, hidden_layer_size_2]))\n","  w3=tf.Variable(tf.random_normal([hidden_layer_size_2, n_classes]))\n","  #biases\n","  b1=tf.Variable(tf.random_normal([hidden_layer_size_1]))\n","  b2=tf.Variable(tf.random_normal([hidden_layer_size_2]))\n","  b3=tf.Variable(tf.random_normal([n_classes]))\n","  #layers\n","  layer_1=activation_function(tf.add(tf.matmul(x, w1), b1))\n","  layer_2=activation_function(tf.add(tf.matmul(layer_1, w2), b2))\n","  y_predicted=tf.matmul(layer_2, w3)+b3\n","\n","\n","  cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predicted, labels=y))\n","  optimizer=optimizer_type(learning_rate=learning_rate).minimize(cost)\n","\n","  session=tf.Session();\n","  session.run(tf.global_variables_initializer())\n","\n","  correct_y_predictediction=tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n","  accuracy=tf.reduce_mean(tf.cast(correct_y_predictediction, tf.float32))\n","\n","  batches_count=int(len(train_images)/batch_size)\n","  accuracies = []\n","\n","  for epoch in range(training_epochs_count):\n","    for i in range(batches_count):\n","      batch_x, batch_y = next_batch(train_images, train_labels, i, batch_size)\n","      batch_x = combine_dims(batch_x, start=1, count=2)\n","      session.run(optimizer, feed_dict={x:batch_x, y:batch_y})\n","    if ((epoch+1)%display_step==0):\n","      acc = session.run(accuracy, feed_dict={x: test_images, y: test_labels})\n","      accuracies.append(acc)\n","        \n","  session.close()\n","\n","  print('Max accuracy:', max(accuracies))\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Accuracy')\n","  plt.plot(accuracies)\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nRutxIGGXHoT"},"source":["\n","\n","| Learning Rate | Batch Size | Activ. Function | Optimizer | Max Accuracy |\n","| ------------- | ---------- | --------------- | --------- | ------------ |\n","| 0.1           | 50         | ReLu            | Adam      | 0.137        |\n","| 0.1           | 50         | Relu            | Gradient  | 0.1029       |\n","| 0.1           | 50         | ReLu            | AdaGrad   | 0.9445       |\n","| 0.1           | 50         | Sigmoid         | Adam      | 0.101        |\n","| 0.1           | 50         | Sigmoid         | Gradient  | 0.8735       |\n","| 0.1           | 50         | Sigmoid         | AdaGrad   | 0.9144       |\n","| 0.1           | 50         | Tanh            | Adam      | 0.1135       |\n","| 0.1           | 50         | Tanh            | Gradient  | 0.6023       |\n","| 0.1           | 50         | Tanh            | AdaGrad   | 0.879        |\n","| 0.1           | 100        | ReLu            | Adam      | 0.2029       |\n","| 0.1           | 100        | Relu            | Gradient  | 0.1135       |\n","| 0.1           | 100        | ReLu            | AdaGrad   | 0.9416       |\n","| 0.1           | 100        | Sigmoid         | Adam      | 0.2683       |\n","| 0.1           | 100        | Sigmoid         | Gradient  | 0.8789       |\n","| 0.1           | 100        | Sigmoid         | AdaGrad   | 0.9081       |\n","| 0.1           | 100        | Tanh            | Adam      | 0.1135       |\n","| 0.1           | 100        | Tanh            | Gradient  | 0.7393       |\n","| 0.1           | 100        | Tanh            | AdaGrad   | 0.8734       |\n","| 0.1           | 500        | ReLu            | Adam      | 0.3304       |\n","| 0.1           | 500        | Relu            | Gradient  | 0.1135       |\n","| 0.1           | 500        | ReLu            | AdaGrad   | 0.9302       |\n","| 0.1           | 500        | Sigmoid         | Adam      | 0.4971       |\n","| 0.1           | 500        | Sigmoid         | Gradient  | 0.8296       |\n","| 0.1           | 500        | Sigmoid         | AdaGrad   | 0.8709       |\n","| 0.1           | 500        | Tanh            | Adam      | 0.4911       |\n","| 0.1           | 500        | Tanh            | Gradient  | 0.7472       |\n","| 0.1           | 500        | Tanh            | AdaGrad   | 0.8269       |\n","| 0.01          | 50         | ReLu            | Adam      | 0.9416       |\n","| 0.01          | 50         | Relu            | Gradient  | 0.098        |\n","| 0.01          | 50         | ReLu            | AdaGrad   | 0.9212       |\n","| 0.01          | 50         | Sigmoid         | Adam      | 0.8805       |\n","| 0.01          | 50         | Sigmoid         | Gradient  | 0.8262       |\n","| 0.01          | 50         | Sigmoid         | AdaGrad   | 0.8052       |\n","| 0.01          | 50         | Tanh            | Adam      | 0.7461       |\n","| 0.01          | 50         | Tanh            | Gradient  | 0.7519       |\n","| 0.01          | 50         | Tanh            | AdaGrad   | 0.7819       |\n","| 0.01          | 100        | ReLu            | Adam      | 0.9682       |\n","| 0.01          | 100        | Relu            | Gradient  | 0.1135       |\n","| 0.01          | 100        | ReLu            | AdaGrad   | 0.9156       |\n","| 0.01          | 100        | Sigmoid         | Adam      | 0.8907       |\n","| 0.01          | 100        | Sigmoid         | Gradient  | 0.7865       |\n","| 0.01          | 100        | Sigmoid         | AdaGrad   | 0.7802       |\n","| 0.01          | 100        | Tanh            | Adam      | 0.8136       |\n","| 0.01          | 100        | Tanh            | Gradient  | 0.7217       |\n","| 0.01          | 100        | Tanh            | AdaGrad   | 0.7726       |\n","| 0.01          | 500        | ReLu            | Adam      | 0.9673       |\n","| 0.01          | 500        | Relu            | Gradient  | 0.098        |\n","| 0.01          | 500        | ReLu            | AdaGrad   | 0.9118       |\n","| 0.01          | 500        | Sigmoid         | Adam      | 0.9169       |\n","| 0.01          | 500        | Sigmoid         | Gradient  | 0.5965       |\n","| 0.01          | 500        | Sigmoid         | AdaGrad   | 0.6958       |\n","| 0.01          | 500        | Tanh            | Adam      | 0.8885       |\n","| 0.01          | 500        | Tanh            | Gradient  | 0.6524       |\n","| 0.01          | 500        | Tanh            | AdaGrad   | 0.6935       |\n","| 0.001         | 50         | ReLu            | Adam      | 0.9585       |\n","| 0.001         | 50         | Relu            | Gradient  | 0.098        |\n","| 0.001         | 50         | ReLu            | AdaGrad   | 0.8008       |\n","| 0.001         | 50         | Sigmoid         | Adam      | 0.9279       |\n","| 0.001         | 50         | Sigmoid         | Gradient  | 0.6241       |\n","| 0.001         | 50         | Sigmoid         | AdaGrad   | 0.5212       |\n","| 0.001         | 50         | Tanh            | Adam      | 0.8908       |\n","| 0.001         | 50         | Tanh            | Gradient  | 0.6658       |\n","| 0.001         | 50         | Tanh            | AdaGrad   | 0.4097       |\n","| 0.001         | 100        | ReLu            | Adam      | 0.9523       |\n","| 0.001         | 100        | Relu            | Gradient  | 0.103        |\n","| 0.001         | 100        | ReLu            | AdaGrad   | 0.7716       |\n","| 0.001         | 100        | Sigmoid         | Adam      | 0.9212       |\n","| 0.001         | 100        | Sigmoid         | Gradient  | 0.4953       |\n","| 0.001         | 100        | Sigmoid         | AdaGrad   | 0.415        |\n","| 0.001         | 100        | Tanh            | Adam      | 0.8748       |\n","| 0.001         | 100        | Tanh            | Gradient  | 0.5969       |\n","| 0.001         | 100        | Tanh            | AdaGrad   | 0.3963       |\n","| 0.001         | 500        | ReLu            | Adam      | 0.9295       |\n","| 0.001         | 500        | Relu            | Gradient  | 0.098        |\n","| 0.001         | 500        | ReLu            | AdaGrad   | 0.7086       |\n","| 0.001         | 500        | Sigmoid         | Adam      | 0.8743       |\n","| 0.001         | 500        | Sigmoid         | Gradient  | 0.2169       |\n","| 0.001         | 500        | Sigmoid         | AdaGrad   | 0.2013       |\n","| 0.001         | 500        | Tanh            | Adam      | 0.8234       |\n","| 0.001         | 500        | Tanh            | Gradient  | 0.328        |\n","| 0.001         | 500        | Tanh            | AdaGrad   | 0.2794       |\n","\n","\n","Ordered by accuracy:\n","\n","| Learning Rate | Batch Size | Activ. Function | Optimizer | Max Accuracy |\n","| ------------- | ---------- | --------------- | --------- | ------------ |\n","| 0.01          | 100        | ReLu            | Adam      | 0.9682       |\n","| 0.01          | 500        | ReLu            | Adam      | 0.9673       |\n","| 0.001         | 50         | ReLu            | Adam      | 0.9585       |\n","| 0.001         | 100        | ReLu            | Adam      | 0.9523       |\n","| 0.1           | 50         | ReLu            | AdaGrad   | 0.9445       |\n","| 0.1           | 100        | ReLu            | AdaGrad   | 0.9416       |\n","| 0.01          | 50         | ReLu            | Adam      | 0.9416       |\n","| 0.1           | 500        | ReLu            | AdaGrad   | 0.9302       |\n","| 0.001         | 500        | ReLu            | Adam      | 0.9295       |\n","| 0.001         | 50         | Sigmoid         | Adam      | 0.9279       |\n","| 0.01          | 50         | ReLu            | AdaGrad   | 0.9212       |\n","| 0.001         | 100        | Sigmoid         | Adam      | 0.9212       |\n","| 0.01          | 500        | Sigmoid         | Adam      | 0.9169       |\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":0}
